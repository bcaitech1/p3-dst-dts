ModelName : TRADE    # {SUMBT, TRADE}

wandb :
    using: True
    project: dohoon_dst
    entity: vail131
    tags: 
        - v0
        
SharedPrams:
    data_dir: /opt/ml/input/data/train_dataset
    eval_data_dir: /opt/ml/input/data/eval_dataset
    model_dir: /opt/ml/p3-dst-dts/dohoon/code/model
    output_dir: /opt/ml/p3-dst-dts/dohoon/code/predictions
    ontology_root : /opt/ml/p3-dst-dts/dohoon/code/results/edit_ontology_metro.json
    
    save_model: True

    use_small_data: True # {True | False} -> True면 테스트용으로 다이얼로그 100짜리로 돌림

    task_name: trade_10ep_proj_dim

    train_log_step: 50
    train_running_loss_len: 50

    use_generation_only: False
    use_gen_dialog_only: False

    use_amp: False
    device_pref: cuda

    adam_epsilon: !!python/float 1e-08
    max_grad_norm: 1.0
    warmup_ratio: 0.1
    weight_decay: 0.01

    random_seed: 42
    
    use_domain_slot: basic # {basic | gen | cat} basic 다 사용,
                            # gen: prepare.py 상위에 있는 gen_slot_meta에 정의된 gen 사용
                            # cat: prepare.py gen_slot_meta에 해당하지 않는 slot-meta 사용
SUMBT:
    hidden_dim: 300
    learning_rate: !!python/float 5e-5


    num_train_epochs: 30
    
    train_batch_size: 4
    eval_batch_size: 4

    model_name_or_path: dsksd/bert-ko-small-minimal
    num_rnn_layers: 1
    zero_init_rnn: False


    max_seq_length: 128
    max_label_length: 12

    attn_head: 8

    fix_utterance_encoder: False

    distance_metric: euclidean
    preprocessor: SUMBTPreprocessor
    model_class: SUMBT

    use_larger_slot_encoding: True  # True -> [max_label_length, emb_dim]만큼 slot_emb에 사용
                                    #    nlp 전체 출력 사용
                                    # False -> emd_dim 만큼 slot_emb에 사용, nlp 출력의 첫번째만 사용
    
    use_transformer: True # True -> transformer, False -> gru
    n_transformers: 2 # transfomer block 몇개 중첩으로 사용할건지

TRADE:
    hidden_size: 768
    learning_rate: !!python/float 0.00003

    num_train_epochs: 10

    train_batch_size: 16
    eval_batch_size: 8

    model_name_or_path: dsksd/bert-ko-small-minimal
    vocab_size: 35000
    hidden_dropout_prob: 0.1
    proj_dim: 2
    teacher_forcing_ratio: 0.5
    n_gate: 5
    preprocessor: TRADEPreprocessor
    model_class: TRADE
