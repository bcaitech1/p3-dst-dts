{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e747e7-f5c2-44fe-80b8-54f919fa9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from data_utils import get_examples_from_dialogues, convert_state_dict, load_dataset\n",
    "from data_utils import OntologyDSTFeature, DSTPreprocessor, _truncate_seq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a098517-9ff1-4774-9c32-a143f14d68eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0+cu101'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f311cb03-6bb6-42cb-b2db-a395e3188ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"/opt/ml/input/data/train_dataset/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/opt/ml/input/data/train_dataset/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/opt/ml/input/data/train_dataset/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e1c1a1-5743-4813-b7c5-b9838ec11865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 8910.14it/s]\n",
      "100%|██████████| 699/699 [00:00<00:00, 16540.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_examples = get_examples_from_dialogues(data=train_data,\n",
    "                                             user_first=True,\n",
    "                                             dialogue_level=True)\n",
    "\n",
    "dev_examples = get_examples_from_dialogues(data=dev_data,\n",
    "                                           user_first=True,\n",
    "                                           dialogue_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f32c756-9847-4d4e-abd2-c1ba6e2801c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSTInputExample(guid='snowy-hat-8324:관광_식당_11-5', context_turns=['', '서울 중앙에 있는 박물관을 찾아주세요', '안녕하세요. 문화역서울 284은 어떠신가요? 평점도 4점으로 방문객들에게 좋은 평가를 받고 있습니다.', '좋네요 거기 평점은 말해주셨구 전화번호가 어떻게되나요?', '전화번호는 983880764입니다. 더 필요하신 게 있으실까요?', '네 관광지와 같은 지역의 한식당을 가고싶은데요 야외석이 있어야되요', '생각하고 계신 가격대가 있으신가요?', '음.. 저렴한 가격대에 있나요?', '죄송하지만 저렴한 가격대에는 없으시네요.', '그럼 비싼 가격대로 다시 찾아주세요'], current_turn=['좋습니당 토요일 18:00에 1명 예약가능한가요?', '외계인의맛집은 어떠신가요? 대표 메뉴는 한정식입니다.'], label=['관광-종류-박물관', '관광-지역-서울 중앙', '관광-이름-문화역서울 284', '식당-가격대-비싼', '식당-지역-서울 중앙', '식당-종류-한식당', '식당-야외석 유무-yes', '식당-예약 요일-토요일', '식당-예약 시간-18:00', '식당-예약 명수-1', '식당-이름-외계인의맛집'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c54cf8-befa-44a1-a45c-92160c0c79bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94097e7d-9d74-49f5-bc64-da8369981b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_turn = max([len(e['dialogue']) for e in train_data])\n",
    "tokenizer = BertTokenizer.from_pretrained('dsksd/bert-ko-small-minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4a2e0ac-c3f3-4649-8678-b55d4926e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from data_utils import get_examples_from_dialogues, convert_state_dict, load_dataset\n",
    "from data_utils import OntologyDSTFeature, DSTPreprocessor, _truncate_seq_pair\n",
    "\n",
    "class SUMBTPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=64,\n",
    "        max_turn_length=14,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.max_turn_length = max_turn_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        guid = example[0].guid.rsplit(\"-\", 1)[0]  # dialogue_idx\n",
    "        turns = []\n",
    "        token_types = []\n",
    "        labels = []\n",
    "        num_turn = None\n",
    "        for turn in example[: self.max_turn_length]:\n",
    "            assert len(turn.current_turn) == 2\n",
    "            uttrs = []\n",
    "            for segment_idx, uttr in enumerate(turn.current_turn):\n",
    "                token = self.src_tokenizer.encode(uttr, add_special_tokens=False)\n",
    "                uttrs.append(token)\n",
    "\n",
    "            _truncate_seq_pair(uttrs[0], uttrs[1], self.max_seq_length - 3)\n",
    "            tokens = (\n",
    "                [self.src_tokenizer.cls_token_id]\n",
    "                + uttrs[0]\n",
    "                + [self.src_tokenizer.sep_token_id]\n",
    "                + uttrs[1]\n",
    "                + [self.src_tokenizer.sep_token_id]\n",
    "            )\n",
    "            token_type = [0] * (len(uttrs[0]) + 2) + [1] * (len(uttrs[1]) + 1)\n",
    "            if len(tokens) < self.max_seq_length:\n",
    "                gap = self.max_seq_length - len(tokens)\n",
    "                tokens.extend([self.src_tokenizer.pad_token_id] * gap)\n",
    "                token_type.extend([0] * gap)\n",
    "            turns.append(tokens)\n",
    "            token_types.append(token_type)\n",
    "            label = []\n",
    "            if turn.label:\n",
    "                slot_dict = convert_state_dict(turn.label)\n",
    "            else:\n",
    "                slot_dict = {}\n",
    "            for slot_type in self.slot_meta:\n",
    "                value = slot_dict.get(slot_type, \"none\")\n",
    "                \n",
    "                label_idx = ontology[slot_type].index(value)\n",
    "               \n",
    "                label.append(label_idx)\n",
    "            labels.append(label)\n",
    "        num_turn = len(turns)\n",
    "        if len(turns) < self.max_turn_length:\n",
    "            gap = self.max_turn_length - len(turns)\n",
    "            for _ in range(gap):\n",
    "                dummy_turn = [self.src_tokenizer.pad_token_id] * self.max_seq_length\n",
    "                turns.append(dummy_turn)\n",
    "                token_types.append(dummy_turn)\n",
    "                dummy_label = [-1] * len(self.slot_meta)\n",
    "                labels.append(dummy_label)\n",
    "        return OntologyDSTFeature(\n",
    "            guid=guid,\n",
    "            input_ids=turns,\n",
    "            segment_ids=token_types,\n",
    "            num_turn=num_turn,\n",
    "            target_ids=labels,\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, tqdm(examples)))\n",
    "\n",
    "    def recover_state(self, pred_slots, num_turn):\n",
    "        states = []\n",
    "        \n",
    "        for pred_slot in pred_slots[:num_turn]:\n",
    "            state = []\n",
    "            for s, p in zip(self.slot_meta, pred_slot):\n",
    "                v = self.ontology[s][p]\n",
    "                if v != 'none':\n",
    "                    state.append(f'{s}-{v}')\n",
    "                    \n",
    "            states.append(state)\n",
    "            \n",
    "        return states\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor([b.input_ids for b in batch])\n",
    "        segment_ids = torch.LongTensor([b.segment_ids for b in batch])\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "        target_ids = torch.LongTensor([b.target_ids for b in batch])\n",
    "        num_turns = [b.num_turn for b in batch]\n",
    "        return input_ids, segment_ids, input_masks, target_ids, num_turns, guids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8714ed3a-f963-4618-aae7-b8feb8b9fe25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0d7353e-0b29-4d53-8613-1981b5e81ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becefe3e4e614114a8e876d6b6d7f0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6301.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a59f81a1c3e4db4bf414eba750e721b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=699.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processor = SUMBTPreprocessor(slot_meta,\n",
    "                              tokenizer,\n",
    "                              ontology=ontology,  # predefined ontology\n",
    "                              max_seq_length=64,  # 각 turn마다 최대 길이\n",
    "                              max_turn_length=max_turn)  # 각 dialogue의 최대 turn 길이\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "355ab239-b480-40d3-843f-cd792a949921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6301\n",
      "699\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))  # 대화 level의 features\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4cadef9-6b4d-40ac-bd17-94237ae4ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Most of code is from https://github.com/SKTBrain/SUMBT\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CosineEmbeddingLoss, CrossEntropyLoss\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "\n",
    "\n",
    "class BertForUtteranceEncoding(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForUtteranceEncoding, self).__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        return self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.scores = None\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        self.scores = scores\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "    def get_scores(self):\n",
    "        return self.scores\n",
    "\n",
    "\n",
    "class SUMBT(nn.Module):\n",
    "    def __init__(self, args, num_labels, device):\n",
    "        super(SUMBT, self).__init__()\n",
    "\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.rnn_num_layers = args.num_rnn_layers\n",
    "        self.zero_init_rnn = args.zero_init_rnn\n",
    "        self.max_seq_length = args.max_seq_length\n",
    "        self.max_label_length = args.max_label_length\n",
    "        self.num_labels = num_labels\n",
    "        self.num_slots = len(num_labels)\n",
    "        self.attn_head = args.attn_head\n",
    "        self.device = device\n",
    "\n",
    "        ### Utterance Encoder\n",
    "        self.utterance_encoder = BertForUtteranceEncoding.from_pretrained(\n",
    "            args.model_name_or_path\n",
    "        )\n",
    "        self.bert_output_dim = self.utterance_encoder.config.hidden_size\n",
    "        self.hidden_dropout_prob = self.utterance_encoder.config.hidden_dropout_prob\n",
    "        if args.fix_utterance_encoder:\n",
    "            for p in self.utterance_encoder.bert.pooler.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        ### slot, slot-value Encoder (not trainable)\n",
    "        self.sv_encoder = BertForUtteranceEncoding.from_pretrained(\n",
    "            args.model_name_or_path\n",
    "        )\n",
    "        # os.path.join(args.bert_dir, 'bert-base-uncased.model'))\n",
    "        for p in self.sv_encoder.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.slot_lookup = nn.Embedding(self.num_slots, self.bert_output_dim)\n",
    "        self.value_lookup = nn.ModuleList(\n",
    "            [nn.Embedding(num_label, self.bert_output_dim) for num_label in num_labels]\n",
    "        )\n",
    "\n",
    "        ### Attention layer\n",
    "        self.attn = MultiHeadAttention(self.attn_head, self.bert_output_dim, dropout=0)\n",
    "\n",
    "        ### RNN Belief Tracker\n",
    "        self.nbt = nn.GRU(\n",
    "            input_size=self.bert_output_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.rnn_num_layers,\n",
    "            dropout=self.hidden_dropout_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.init_parameter(self.nbt)\n",
    "\n",
    "        if not self.zero_init_rnn:\n",
    "            self.rnn_init_linear = nn.Sequential(\n",
    "                nn.Linear(self.bert_output_dim, self.hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.hidden_dropout_prob),\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.bert_output_dim)\n",
    "        self.layer_norm = nn.LayerNorm(self.bert_output_dim)\n",
    "\n",
    "        ### Measure\n",
    "        self.metric = torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)\n",
    "\n",
    "        ### Classifier\n",
    "        self.nll = CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "        ### Etc.\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "    def initialize_slot_value_lookup(self, label_ids, slot_ids):\n",
    "\n",
    "        self.sv_encoder.eval()\n",
    "\n",
    "        # Slot encoding\n",
    "        slot_type_ids = torch.zeros(slot_ids.size(), dtype=torch.long).to(\n",
    "            slot_ids.device\n",
    "        )\n",
    "        slot_mask = slot_ids > 0\n",
    "        hid_slot, _ = self.sv_encoder(\n",
    "            slot_ids.view(-1, self.max_label_length),\n",
    "            slot_type_ids.view(-1, self.max_label_length),\n",
    "            slot_mask.view(-1, self.max_label_length),\n",
    "        )\n",
    "        hid_slot = hid_slot[:, 0, :]\n",
    "        hid_slot = hid_slot.detach()\n",
    "        self.slot_lookup = nn.Embedding.from_pretrained(hid_slot, freeze=True)\n",
    "\n",
    "        for s, label_id in enumerate(label_ids):\n",
    "            label_type_ids = torch.zeros(label_id.size(), dtype=torch.long).to(\n",
    "                label_id.device\n",
    "            )\n",
    "            label_mask = label_id > 0\n",
    "            hid_label, _ = self.sv_encoder(\n",
    "                label_id.view(-1, self.max_label_length),\n",
    "                label_type_ids.view(-1, self.max_label_length),\n",
    "                label_mask.view(-1, self.max_label_length),\n",
    "            )\n",
    "            hid_label = hid_label[:, 0, :]\n",
    "            hid_label = hid_label.detach()\n",
    "            self.value_lookup[s] = nn.Embedding.from_pretrained(hid_label, freeze=True)\n",
    "            self.value_lookup[s].padding_idx = -1\n",
    "\n",
    "        print(\"Complete initialization of slot and value lookup\")\n",
    "        self.sv_encoder = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        token_type_ids,\n",
    "        attention_mask,\n",
    "        labels=None,\n",
    "        n_gpu=1,\n",
    "        target_slot=None,\n",
    "    ):\n",
    "        # B = Batch Size\n",
    "        # M = Max Turn Size\n",
    "        # N = Seq Len\n",
    "        # J = Target_slot Len\n",
    "        # H_GRU = RNN Hidden Dim\n",
    "        \n",
    "        # input_ids: [B, M, N]\n",
    "        # token_type_ids: [B, M, N]\n",
    "        # attention_mask: [B, M, N]\n",
    "        # labels: [B, M, J]\n",
    "\n",
    "        # if target_slot is not specified, output values corresponding all slot-types\n",
    "        if target_slot is None:\n",
    "            target_slot = list(range(0, self.num_slots))\n",
    "\n",
    "        ds = input_ids.size(0)  # Batch size (B)\n",
    "        ts = input_ids.size(1)  # Max turn size (M)\n",
    "        bs = ds * ts # B * M\n",
    "        slot_dim = len(target_slot)  # J\n",
    "\n",
    "        # Utterance encoding\n",
    "        hidden, _ = self.utterance_encoder(\n",
    "            input_ids.view(-1, self.max_seq_length),\n",
    "            token_type_ids.view(-1, self.max_seq_length),\n",
    "            attention_mask.view(-1, self.max_seq_length),\n",
    "        ) # [B*M, N, H]\n",
    "\n",
    "        hidden = torch.mul(\n",
    "            hidden,\n",
    "            attention_mask.view(-1, self.max_seq_length, 1)\n",
    "            .expand(hidden.size())\n",
    "            .float(),\n",
    "        )\n",
    "        hidden = hidden.repeat(slot_dim, 1, 1)  # [J*M*B, N, H]\n",
    "\n",
    "        hid_slot = self.slot_lookup.weight[\n",
    "            target_slot, :\n",
    "        ]  # Select target slot embedding # [J, H]\n",
    "        hid_slot = hid_slot.repeat(1, bs).view(bs * slot_dim, -1)  # [J*M*B, H]\n",
    "\n",
    "        # Attended utterance vector\n",
    "        hidden = self.attn(\n",
    "            hid_slot,  # q^s  [J*M*B, H] => [J*M*B, 1, H]\n",
    "            hidden,  # U [J*M*B, N, H]\n",
    "            hidden,  # U [J*M*B, N, H]\n",
    "            mask=attention_mask.view(-1, 1, self.max_seq_length).repeat(slot_dim, 1, 1),\n",
    "        ) # [J*M*B, 1, H] -> 1 = hid_slot_seq_len\n",
    "        hidden = hidden.squeeze()  # h [J*M*B, H] Aggregated Slot Context\n",
    "        hidden = hidden.view(slot_dim, ds, ts, -1).view(\n",
    "            -1, ts, self.bert_output_dim\n",
    "        )  # [J*B, M, H]\n",
    "\n",
    "        # NBT\n",
    "        if self.zero_init_rnn:\n",
    "            h = torch.zeros(\n",
    "                self.rnn_num_layers, input_ids.shape[0] * slot_dim, self.hidden_dim\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [1, slot_dim*ds, hidden]\n",
    "        else:\n",
    "            h = hidden[:, 0, :].unsqueeze(0).expand(self.rnn_num_layers, -1, -1) # 원래 repeat(self.rnn_num_layers, 1, 1) 이었는데 바꿈\n",
    "            h = self.rnn_init_linear(h)    # 왜냐면 1 dim은 언제나 사이즈 1이니까 expand 사용하는게 더 좋음(expand 메모리 복사 없음, repeat 복사됨)\n",
    "\n",
    "        if isinstance(self.nbt, nn.GRU):\n",
    "            rnn_out, _ = self.nbt(hidden, h)  # [J*B, M, H_GRU]\n",
    "        elif isinstance(self.nbt, nn.LSTM):\n",
    "            c = torch.zeros(\n",
    "                self.rnn_num_layers, input_ids.shape[0] * slot_dim, self.hidden_dim\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [1, slot_dim*ds, hidden]\n",
    "            rnn_out, _ = self.nbt(hidden, (h, c))  # [slot_dim*ds, turn, hidden]\n",
    "        rnn_out = self.layer_norm(self.linear(self.dropout(rnn_out)))\n",
    "\n",
    "        hidden = rnn_out.view(slot_dim, ds, ts, -1)  # [J, B, M, H]  변경 전 # [J, B, M, H_G]\n",
    "\n",
    "        # Label (slot-value) encoding\n",
    "        loss = 0\n",
    "        loss_slot = []\n",
    "        pred_slot = []\n",
    "        output = []\n",
    "        for s, slot_id in enumerate(target_slot):  ## note: target_slots are successive\n",
    "            # loss calculation\n",
    "            hid_label = self.value_lookup[slot_id].weight\n",
    "            num_slot_labels = hid_label.size(0)\n",
    "\n",
    "            _hid_label = (\n",
    "                hid_label.unsqueeze(0)\n",
    "                .unsqueeze(0)\n",
    "               .repeat(ds, ts, 1, 1)\n",
    "                .view(ds * ts * num_slot_labels, -1)\n",
    "            )\n",
    "            _hidden = (\n",
    "                hidden[s, :, :, :]\n",
    "                .unsqueeze(2)\n",
    "                .repeat(1, 1, num_slot_labels, 1)\n",
    "                .view(ds * ts * num_slot_labels, -1)\n",
    "            )\n",
    "            _dist = self.metric(_hid_label, _hidden).view(ds, ts, num_slot_labels)\n",
    "            _dist = -_dist\n",
    "            _, pred = torch.max(_dist, -1)\n",
    "            pred_slot.append(pred.view(ds, ts, 1))\n",
    "            output.append(_dist)\n",
    "\n",
    "            if labels is not None:\n",
    "                _loss = self.nll(_dist.view(ds * ts, -1), labels[:, :, s].view(-1))\n",
    "                loss_slot.append(_loss.item())\n",
    "                loss += _loss\n",
    "\n",
    "        pred_slot = torch.cat(pred_slot, 2)\n",
    "        \n",
    "        if labels is None:\n",
    "            return output, pred_slot\n",
    "\n",
    "        # calculate joint accuracy\n",
    "        accuracy = (pred_slot == labels).view(-1, slot_dim)\n",
    "        acc_slot = (\n",
    "            torch.sum(accuracy, 0).float()\n",
    "            / torch.sum(labels.view(-1, slot_dim) > -1, 0).float()\n",
    "        )\n",
    "        acc = (\n",
    "            sum(torch.sum(accuracy, 1) / slot_dim).float()\n",
    "            / torch.sum(labels[:, :, 0].view(-1) > -1, 0).float()\n",
    "        )  # joint accuracy\n",
    "        \n",
    "#         acc2 = ( # 별 차이 없으니까 그냥 기존거 쓰자\n",
    "#             torch.sum(accuracy).float()\n",
    "#             / torch.sum(labels > -1).float()\n",
    "#         )  # joint accuracy\n",
    "#         assert acc == acc2, f'acc: {acc}  acc2: {acc2}'\n",
    "\n",
    "        if n_gpu == 1:\n",
    "            return loss, loss_slot, acc, acc_slot, pred_slot\n",
    "        else:\n",
    "            return (\n",
    "                loss.unsqueeze(0),\n",
    "                None,\n",
    "                acc.unsqueeze(0),\n",
    "                acc_slot.unsqueeze(0),\n",
    "                pred_slot.unsqueeze(0),\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def init_parameter(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(module.weight)\n",
    "            torch.nn.init.constant_(module.bias, 0.0)\n",
    "        elif isinstance(module, nn.GRU) or isinstance(module, nn.LSTM):\n",
    "            torch.nn.init.xavier_normal_(module.weight_ih_l0)\n",
    "            torch.nn.init.xavier_normal_(module.weight_hh_l0)\n",
    "            torch.nn.init.constant_(module.bias_ih_l0, 0.0)\n",
    "            torch.nn.init.constant_(module.bias_hh_l0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91624846-fb62-4022-bbea-d2fdcfad8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def tokenize_ontology(ontology, tokenizer, max_seq_length):\n",
    "    slot_types = []\n",
    "    slot_values = []\n",
    "    for k, v in ontology.items():\n",
    "        tokens = tokenizer.encode(k)\n",
    "        if len(tokens) < max_seq_length:\n",
    "            gap = max_seq_length - len(tokens)\n",
    "            tokens.extend([tokenizer.pad_token_id] * gap)\n",
    "        slot_types.append(tokens)\n",
    "        slot_value = []\n",
    "        for vv in v:\n",
    "            tokens = tokenizer.encode(vv)\n",
    "            if len(tokens) < max_seq_length:\n",
    "                gap = max_seq_length - len(tokens)\n",
    "                tokens.extend([tokenizer.pad_token_id] * gap)\n",
    "            slot_value.append(tokens)\n",
    "        slot_values.append(torch.LongTensor(slot_value))\n",
    "    return torch.LongTensor(slot_types), slot_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bcb68f-465d-4336-9fe9-365b72eeaf6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Slot:  torch.Size([45, 12])\n",
      "Tokenized Value of 관광-경치 좋은 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-교육적 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-도보 가능 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-문화 예술 torch.Size([4, 12])\n",
      "Tokenized Value of 관광-역사적 torch.Size([4, 12])\n"
     ]
    }
   ],
   "source": [
    "slot_type_ids, slot_values_ids = tokenize_ontology(ontology, tokenizer, 12)\n",
    "num_labels = [len(s) for s in slot_values_ids]\n",
    "\n",
    "print('Tokenized Slot: ', slot_type_ids.size())\n",
    "cnt = 0\n",
    "for slot, slot_value_id in zip(slot_meta, slot_values_ids):\n",
    "    print(f'Tokenized Value of {slot}', slot_value_id.size())\n",
    "    cnt += 1\n",
    "    if cnt > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec836a44-adf0-4960-8942-5ffc301a5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = {\n",
    "    'hidden_dim': 300,\n",
    "    'num_rnn_layers': 2,\n",
    "    'zero_init_rnn': False,\n",
    "    'max_seq_length': 64,\n",
    "    'max_label_length': 12,\n",
    "    'attn_head': 4,\n",
    "    'fix_utterance_encoder': False,\n",
    "    'task_name': 'sumbtgru',\n",
    "    'distance_metric': 'euclidean',\n",
    "    'model_name_or_path': 'dsksd/bert-ko-small-minimal',\n",
    "    'warmup_ratio': 0.1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_train_epochs': 10\n",
    "}\n",
    "\n",
    "args = Namespace(**args)\n",
    "\n",
    "num_labels = [len(s) for s in slot_values_ids]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = 1 if torch.cuda.device_count() < 2 else torch.cuda.device_count()\n",
    "n_epochs = args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c7e879-4dbd-48f6-ae88-f6d3218a9f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dsksd/bert-ko-small-minimal were not used when initializing BertForUtteranceEncoding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dsksd/bert-ko-small-minimal were not used when initializing BertForUtteranceEncoding: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForUtteranceEncoding from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete initialization of slot and value lookup\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = SUMBT(args, num_labels, device)\n",
    "model.initialize_slot_value_lookup(slot_values_ids, slot_type_ids)  # Tokenized Ontology의 Pre-encoding using BERT_SV\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1abd0a19-6cc2-4edf-9d01-3ad5a3cf2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import WOSDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import random\n",
    "\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=8, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aaf7dda-5cd8-4421-ae9e-07e6bab37a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(t_total * args.warmup_ratio), num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9dff834-09d3-42ab-9bcc-b9736a7fc516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import _evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0eb8f58-45dc-4d9e-bb20-3694ade6ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, eval_loader, processor, device):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    for step, batch in tqdm(enumerate(eval_loader), total=len(eval_loader)):\n",
    "        input_ids, segment_ids, input_masks, target_ids, num_turns, guids  = \\\n",
    "            [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, pred_slot = model(input_ids, segment_ids, input_masks, None, 1)\n",
    "            \n",
    "        pred_slot = pred_slot.detach().cpu()\n",
    "        \n",
    "        for guid, num_turn, p_slot in zip(guids, num_turns, pred_slot):\n",
    "            pred_states = processor.recover_state(p_slot, num_turn)\n",
    "            for t_idx, pred_state in enumerate(pred_states):\n",
    "                predictions[f'{guid}-{t_idx}'] = pred_state\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a76acc85-ce63-49c1-bbb3-f6ebb2de64a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6b51630634ce69d3fc91492bc1e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a02f10828eb4ef1be2ed5db00a49ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=788.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/788] 127.667603\n",
      "[0/10] [100/788] 43.364773\n",
      "[0/10] [200/788] 47.441975\n",
      "[0/10] [300/788] 39.628750\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b0b2df8b5b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score, best_checkpoint = 0, 0\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        input_ids, segment_ids, input_masks, target_ids, num_turns, guids  = \\\n",
    "        [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "\n",
    "        # Forward\n",
    "        if n_gpu == 1:\n",
    "            loss, loss_slot, acc, acc_slot, _ = model(input_ids, segment_ids, input_masks, target_ids, n_gpu)\n",
    "        else:\n",
    "            loss, _, acc, acc_slot, _ = model(input_ids, segment_ids, input_masks, target_ids, n_gpu)\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "            \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e93b2-21a5-40d2-a03d-5ac8a7528604",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=True, dialogue_level=True\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff35616-a61d-4372-88e0-2272c8b6f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a6a91-639c-4586-9d59-49a858244f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions/predictions.csv', 'w'), indent=2, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74ccb2-2123-4378-8c6d-9447cabe0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe4aa5c-dc16-4a75-8a7f-7f1e5c524178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfde8c1-ce26-40cc-96e4-3cc458cca625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
