ModelName : SUMBT

SUMBT:
    hidden_dim: 300
    learning_rate: !!python/float 5e-5
    warmup_ratio: 0.1
    weight_decay: 0.01
    num_train_epochs: 10
    data_dir: /opt/ml/input/data/train_dataset
    model_dir: ./results
    train_batch_size: 4
    eval_batch_size: 4
    adam_epsilon: !!python/float 1e-08
    max_grad_norm: 1.0
    num_train_epochs: 10
    random_seed: 42
    use_amp: True
    model_name_or_path: dsksd/bert-ko-small-minimal
    num_rnn_layers: 1
    zero_init_rnn: False
    max_seq_length: 128
    max_label_length: 12
    attn_head: 4
    fix_utterance_encoder: False
    task_name: sumbtgru
    distance_metric: euclidean
    preprocessor: SUMBTPreprocessor
    model_class: SUMBT
    device_pref: cuda

    use_larger_slot_encoding: True  # True -> [max_label_length, emb_dim]만큼 slot_emb에 사용
                                    #    nlp 전체 출력 사용
                                    # False -> emd_dim 만큼 slot_emb에 사용, nlp 출력의 첫번째만 사용
    use_transformer: True # True -> transformer, False -> gru
    n_transformers: 4 # transfomer block 몇개 중첩으로 사용할건지

TRADE:
    hidden_size: 768
    learning_rate: 0.0001
    warmup_ratio: 0.1
    weight_decay: 0.01
    num_train_epochs: 10
    data_dir: /opt/ml/input/data/train_dataset
    model_dir: ./results
    train_batch_size: 16
    eval_batch_size: 32
    adam_epsilon: !!python/float 1e-08
    max_grad_norm: 1.0
    num_train_epochs: 30
    random_seed: 42
    use_amp: True
    model_name_or_path: monologg/koelectra-base-v3-discriminator
    vocab_size: 35000
    hidden_dropout_prob: 0.1
    proj_dim: null
    teacher_forcing_ratio: 0.5
    n_gate: 3
    preprocessor: TRADEPreprocessor
    model_class: TRADE
    device_pref: cuda