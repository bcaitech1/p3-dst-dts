ModelName : SUMBT    # {SUMBT, TRADE}

wandb :
    using: False
    project: dst
    entity: rolypolyvg295
    tags: 
        - v0
        - trade
        - decoder_stuff

SharedPrams:
    data_dir: /opt/ml/input/data/train_dataset

    eval_data_dir: /opt/ml/input/data/eval_dataset

    #train시에 나오는 결과물(eda 결과_graph dir, model bin file, inference에서 쓰는 conf)들을 저장
    train_result_dir: /opt/ml/git/p3-dst-dts/team/code/results 

    #prediction files만 저장
    output_dir: /opt/ml/results

    task_name: SUMBT_Transformer_catslot

    save_model: True

    use_small_data: True # {True | False} -> True면 테스트용으로 다이얼로그 100짜리로 돌림

    train_log_step: 50
    train_running_loss_len: 50

    use_generation_only: False
    use_gen_dialog_only: False

    use_amp: True
    device_pref: cuda

    adam_epsilon: !!python/float 1e-08
    max_grad_norm: 1.0
    warmup_ratio: 0.1
    weight_decay: 0.01
    random_seed: 42
    ontology_root: /opt/ml/input/data/train_dataset/edit_ontology_metro.json #edit_ontology_metro.json

    use_domain_slot: cat # {basic | gen | cat} basic 다 사용,
                            # gen: prepare.py 상위에 있는 gen_slot_meta에 정의된 gen 사용
                            # cat: prepare.py gen_slot_meta에 해당하지 않는 slot-meta 사용
    use_convert_ont: False
    convert_time: time_hour_min # {none | time_hour_min}
                                # none: 사용안함
                                # time_hour_min: 시간 xx시 xx분
SUMBT:
    hidden_dim: 300
    learning_rate: !!python/float 5e-5
    num_train_epochs: 10
    train_batch_size: 4
    eval_batch_size: 4
    model_name_or_path: dsksd/bert-ko-small-minimal
    num_rnn_layers: 1
    zero_init_rnn: False

    max_seq_length: 128
    max_label_length: 12
    attn_head: 4
    fix_utterance_encoder: False

    distance_metric: euclidean
    preprocessor: SUMBTPreprocessor
    model_class: SUMBT
    use_larger_slot_encoding: True  # True -> [max_label_length, emb_dim]만큼 slot_emb에 사용
                                    #    nlp 전체 출력 사용
                                    # False -> emd_dim 만큼 slot_emb에 사용, nlp 출력의 첫번째만 사용
    use_mean_value_encoding: False # 사용안하는게 이득
    use_linear_distance_weight: False # 애매, 아주 아주 살짝 좋아짐
    
    use_transformer: True # True -> transformer, False -> gru
    n_transformers: 4 # transfomer block 몇개 중첩으로 사용할건지

TRADE:
    hidden_size: 768
    learning_rate: !!python/float 1e-04

    num_train_epochs: 30

    train_batch_size: 8
    eval_batch_size: 8

    model_name_or_path: dsksd/bert-ko-small-minimal
    vocab_size: 35000
    hidden_dropout_prob: 0.1
    proj_dim: null
    teacher_forcing_ratio: 0.5
    n_gate: 5
    preprocessor: TRADEPreprocessor
    model_class: TRADE

    use_decoder_ts: True
    decoder_n_heads: 4
    decoder_n_layers: 2
