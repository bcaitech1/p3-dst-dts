ModelName : TRADE    # {SUMBT, TRADE}

wandb :
    using: False
    project: dst
    entity: rolypolyvg295
    tags: 
        - v0
        - trade
        - decoder_stuff

SharedPrams:
    data_dir: /opt/ml/input/data/train_dataset
    model_dir: /opt/ml/project/team/code/results
    output_dir: /opt/ml/project/team/code/outputs

    save_model: True
    use_small_data: False # {True | False} -> True면 테스트용으로 다이얼로그 100짜리로 돌림
    train_log_step: 50
    train_running_loss_len: 50
    use_amp: False
    device_pref: cuda
    adam_epsilon: !!python/float 1e-08
    max_grad_norm: 1.0
    warmup_ratio: 0.1
    weight_decay: 0.01
    random_seed: 42
    ontology_root: /opt/ml/input/data/train_dataset/ontology.json #edit_ontology_metro.json
    use_extra_ontology: False # {True | True}
    use_domain_slot: gen # {basic | gen | cat} basic 다 사용,
                            # gen: prepare.py 상위에 있는 gen_slot_meta에 정의된 gen 사용
                            # cat: prepare.py gen_slot_meta에 해당하지 않는 slot-meta 사용
SUMBT:
    hidden_dim: 300
    learning_rate: !!python/float 5e-5
    num_train_epochs: 10
    train_batch_size: 4
    eval_batch_size: 4
    model_name_or_path: dsksd/bert-ko-small-minimal
    num_rnn_layers: 1
    zero_init_rnn: False
    max_seq_length: 128
    max_label_length: 12
    attn_head: 4
    fix_utterance_encoder: False
    task_name: sumbtgru
    distance_metric: euclidean
    preprocessor: SUMBTPreprocessor
    model_class: SUMBT
    use_larger_slot_encoding: True  # True -> [max_label_length, emb_dim]만큼 slot_emb에 사용
                                    #    nlp 전체 출력 사용
                                    # False -> emd_dim 만큼 slot_emb에 사용, nlp 출력의 첫번째만 사용
    use_mean_value_encoding: False # 사용안하는게 이득
    use_linear_distance_weight: False # 애매, 아주 아주 살짝 좋아짐
    
    use_transformer: True # True -> transformer, False -> gru
    n_transformers: 2 # transfomer block 몇개 중첩으로 사용할건지

TRADE:
    hidden_size: 768
    learning_rate: !!python/float 1e-04

    num_train_epochs: 10

    train_batch_size: 16
    eval_batch_size: 8

    model_name_or_path: monologg/koelectra-base-v3-discriminator
    vocab_size: 35000
    hidden_dropout_prob: 0.1
    proj_dim: null
    teacher_forcing_ratio: 0.5
    n_gate: 3
    preprocessor: TRADEPreprocessor
    model_class: TRADE

    use_decoder_ts: True
    decoder_n_heads: 4
    decoder_n_layers: 2